{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# temp_image = Image.open('datasets/FLIR/train/PreviewData/FLIR_00011.jpeg')\n",
    "# print(temp_image.size)\n",
    "# temp_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ../PreviewData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json # import json module\n",
    "\n",
    "# with open('datasets/FLIR/train/Annotations/FLIR_00014.json') as f:\n",
    "#     json_data = json.load(f)\n",
    "\n",
    "# #print(json_data['annotation'])\n",
    "\n",
    "# for i in range(len(json_data['annotation'])):\n",
    "#     print(json_data['annotation'][i]['category_id'])\n",
    "#     print(json_data['annotation'][i]['bbox'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# import numpy\n",
    "# plt.figure(figsize = (15,15))\n",
    "# # tensor([[317.0266,  65.5659, 453.2461, 347.0377],\n",
    "# #         [169.0079,  27.5498, 244.5284, 313.9996]\n",
    "\n",
    "# temp_image = Image.open('datasets/FLIR/train/PreviewData/FLIR_00014.jpeg')\n",
    "# plt.imshow(temp_image)\n",
    "# box = [0,0,0,0]\n",
    "\n",
    "# for i in range(len(json_data['annotation'])):\n",
    "#    box[0], box[1],box[2], box[3] =  json_data['annotation'][i]['bbox']\n",
    "#    box[2] +=box[0]\n",
    "#    box[3]+= box[1]\n",
    "#    x_values = [box[0], box[0], box[2], box[2], box[0]]\n",
    "#    y_values = [box[1], box[3], box[3], box[1], box[1]]\n",
    "#    plt.plot(x_values, y_values, linewidth = 3, label ='asdf')\n",
    "#    #print(json_data['annotation'][i]['category_id'])\n",
    "\n",
    "# # The_boxes = (prediction[0]['boxes']).tolist()\n",
    "# # for box in The_boxes:\n",
    "# #   x_values = [box[0], box[0], box[2], box[2], box[0]]\n",
    "# #   y_values = [box[1], box[3], box[3], box[1], box[1]]\n",
    "# #   plt.plot(x_values, y_values, linewidth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PreviewData\"))))\n",
    "        self.jsons = list(sorted(os.listdir(os.path.join(root, \"Annotations\"))))\n",
    "        #self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "        self.Kimg = []\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PreviewData\", self.imgs[idx])\n",
    "        json_path = os.path.join(self.root, \"Annotations\", self.jsons[idx])\n",
    "        #mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        with open(json_path) as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        area = []\n",
    "        if len(json_data['annotation']) <1:\n",
    "            jpegSTR = str(json_data['image']['file_name']) + '.jpeg'\n",
    "            jsonSTR = str(json_data['image']['file_name']) + '.json'\n",
    "            \n",
    "            !mv datasets/FLIR/train/PreviewData/{jpegSTR} datasets/FLIR/train/NotData/\n",
    "            !mv datasets/FLIR/train/Annotations/{jsonSTR} datasets/FLIR/train/NotData/\n",
    "            return str(json_data['image'])\n",
    "        else:\n",
    "            box = [0,0,0,0]\n",
    "        \n",
    "        \n",
    "        for i in range(len(json_data['annotation'])):\n",
    "            # true인 곳의 위치들이 array로 pos에 들어감\n",
    "            # where : 괄호 안에 참값이 들어갈 때 그 index값을 반환해줌 , 즉 좌표를 반환해줌 pos[0]은 y값들의 위치 pos[1]은 x값들의 위치\n",
    "            box[0], box[1],box[2], box[3] =  json_data['annotation'][i]['bbox']\n",
    "            box[2] +=box[0]\n",
    "            box[3]+= box[1]\n",
    "            xmin = (box[0])\n",
    "            xmax = (box[2])\n",
    "            ymin = (box[1])\n",
    "            ymax = (box[3])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            area.append((xmax - xmin) * (ymax-ymin))\n",
    "            if (int(json_data['annotation'][i]['category_id']) > 4):\n",
    "                labels.append(5)\n",
    "            else:\n",
    "                labels.append(int(json_data['annotation'][i]['category_id']))\n",
    "            \n",
    "            \n",
    "            if not (int(json_data['annotation'][i]['category_id']) in self.Kimg):\n",
    "                #print(int(json_data['annotation'][i]['category_id']))\n",
    "                self.Kimg.append(int(json_data['annotation'][i]['category_id']))\n",
    "        \n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "#        print(\"i : \", i)\n",
    "        #labels = labels[0]\n",
    "        labels = torch.tensor((labels), dtype=torch.int64)\n",
    "#        print(labels)\n",
    "#        labels = torch.ones((len(labels),), dtype=torch.int64)\n",
    "\n",
    "#        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        \n",
    "     #   area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # 그냥 가로 * 세로 로 면적 계산\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((len(json_data['annotation']),), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    " #       target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PennFudanDataset('datasets/FLIR/train')\n",
    "print(len(dataset))\n",
    "# for i in ((dataset)):\n",
    "#     i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "      \n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # COCO에서 미리 학습된 인스턴스 분할 모델을 읽어옵니다.\n",
    "    #model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # 분류를 위한 입력 특징 차원을 얻습니다.\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    #in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    \n",
    "    # hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    #model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "#                                                        hidden_layer,\n",
    "#                                                        num_classes)\n",
    "\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download TorchVision repo to use some files from\n",
    "# references/detection\n",
    "# !git clone https://github.com/pytorch/vision.git\n",
    "# %cd vision\n",
    "# !git checkout v0.3.0\n",
    "\n",
    "# !cp references/detection/utils.py ../\n",
    "# !cp references/detection/transforms.py ../\n",
    "# !cp references/detection/coco_eval.py ../\n",
    "# !cp references/detection/engine.py ../\n",
    "# !cp references/detection/coco_utils.py ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%shell\n",
    "\n",
    "# #Install pycocotools\n",
    "# !git clone https://github.com/cocodataset/cocoapi.git\n",
    "# %cd cocoapi/PythonAPI\n",
    "# !python setup.py build_ext install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset('datasets/FLIR/train', get_transform(train=True))\n",
    "dataset_test = PennFudanDataset('datasets/FLIR/train', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=4, shuffle=True, num_workers=2,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=2, shuffle=False, num_workers=1,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'datasets/FLIR/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DIVICES\"] = '0, 1, 2, 3'\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#device = torch.device(\"cuda:0,1,2,3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 6\n",
    "\n",
    "# get the model using our helper function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "model.load_state_dict(torch.load(\"./checkpoints/epoch?????.pth\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if torch.cuda.device_count() >1:\n",
    "#     print(\"Let's use \", torch.cuda.device_count(),\"GPUs!\")\n",
    "#     model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.00005,\n",
    "                             weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 343\n",
    "\n",
    "path = \"./checkpoints/epoch\" + str(epoch) + \".pth\"\n",
    "print(\"save : \", path)\n",
    "torch.save(model.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train it for 10 epochs\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(30, num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "    if(epoch % 4 == 0):\n",
    "        path = \"./checkpoints/epoch\" + str(epoch) + \".pth\"\n",
    "        print(\"save : \", path)\n",
    "        torch.save(model.state_dict(), path)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test2 = PennFudanDataset('datasets/FLIR/train', get_transform(train=False))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.cuda.device_count())\n",
    "# model = get_instance_segmentation_model(num_classes)\n",
    "\n",
    "# model = torch.nn.DataParallel(model, device_ids=[0])\n",
    "# model.to(device)\n",
    "\n",
    "# device_ = torch.device(\"cuda\")\n",
    "\n",
    "test_nums = 1\n",
    "\n",
    "\n",
    "# pick one image from the test set\n",
    "img, _ = dataset_test[1]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy\n",
    "plt.figure(figsize = (15,15))\n",
    "# tensor([[317.0266,  65.5659, 453.2461, 347.0377],\n",
    "#         [169.0079,  27.5498, 244.5284, 313.9996]\n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "color_set = ['w','r','b','y','g','m']\n",
    "The_boxes = (prediction[0]['boxes']).tolist()\n",
    "\n",
    "this_labels = (prediction[0]['labels']).tolist()\n",
    "\n",
    "\n",
    "for i, box in enumerate(The_boxes):\n",
    "  x_values = [box[0], box[0], box[2], box[2], box[0]]\n",
    "  y_values = [box[1], box[3], box[3], box[1], box[1]]\n",
    "  plt.plot(x_values, y_values, linewidth = 3, color = color_set[this_labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy\n",
    "plt.figure(figsize = (15,15))\n",
    "# tensor([[317.0266,  65.5659, 453.2461, 347.0377],\n",
    "#         [169.0079,  27.5498, 244.5284, 313.9996]\n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "color_set = ['w','r','b','y','g','m']\n",
    "The_boxes = (dataset_test[test_nums][1]['boxes']).tolist()\n",
    "\n",
    "this_labels = (dataset_test[test_nums][1]['labels']).tolist()\n",
    "\n",
    "\n",
    "for i, box in enumerate(The_boxes):\n",
    "  x_values = [box[0], box[0], box[2], box[2], box[0]]\n",
    "  y_values = [box[1], box[3], box[3], box[1], box[1]]\n",
    "  plt.plot(x_values, y_values, linewidth = 3, color = color_set[this_labels[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
